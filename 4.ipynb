{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"参考文献：https://github.com/pytorch/vision\"\"\"\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "#Anchor生成\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "#feature mapから、region proposalにあたる部分領域を抽出する\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(1280, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(1280, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=62720, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上記のFasterRCNNのプログラム内のRPN処理について"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "#rpn.py\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import torchvision\n",
    "from torchvision.ops import boxes as box_ops\n",
    "\n",
    "from . import _utils as det_utils\n",
    "from .image_list import ImageList\n",
    "\n",
    "from torch.jit.annotations import List, Optional, Dict, Tuple\n",
    "\n",
    "\n",
    "@torch.jit.unused\n",
    "def _onnx_get_num_anchors_and_pre_nms_top_n(ob, orig_pre_nms_top_n):\n",
    "    # type: (Tensor, int) -> Tuple[int, int]\n",
    "    from torch.onnx import operators\n",
    "    num_anchors = operators.shape_as_tensor(ob)[1].unsqueeze(0)\n",
    "    pre_nms_top_n = torch.min(torch.cat(\n",
    "        (torch.tensor([orig_pre_nms_top_n], dtype=num_anchors.dtype),\n",
    "         num_anchors), 0))\n",
    "\n",
    "    return num_anchors, pre_nms_top_n\n",
    "\n",
    "#Anchorの生成\n",
    "class AnchorGenerator(nn.Module):\n",
    "    __annotations__ = {\n",
    "        \"cell_anchors\": Optional[List[torch.Tensor]],\n",
    "        \"_cache\": Dict[str, List[torch.Tensor]]\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes=(128, 256, 512),\n",
    "        aspect_ratios=(0.5, 1.0, 2.0),\n",
    "    ):\n",
    "        super(AnchorGenerator, self).__init__()\n",
    "\n",
    "        if not isinstance(sizes[0], (list, tuple)):\n",
    "            # TODO change this\n",
    "            sizes = tuple((s,) for s in sizes)\n",
    "        if not isinstance(aspect_ratios[0], (list, tuple)):\n",
    "            aspect_ratios = (aspect_ratios,) * len(sizes)\n",
    "\n",
    "        assert len(sizes) == len(aspect_ratios)\n",
    "\n",
    "        self.sizes = sizes\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.cell_anchors = None\n",
    "        self._cache = {}\n",
    "\n",
    "\n",
    "    #すべてのアスペクト比とスケールの組み合わせのアンカーを生成　アスペクト比=高さ/幅\n",
    "    def generate_anchors(self, scales, aspect_ratios, dtype=torch.float32, device=\"cpu\"):\n",
    "        # type: (List[int], List[float], int, Device) -> Tensor  # noqa: F821\n",
    "        scales = torch.as_tensor(scales, dtype=dtype, device=device)\n",
    "        aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        return base_anchors.round()\n",
    "\n",
    "    \n",
    "    def set_cell_anchors(self, dtype, device):\n",
    "        # type: (int, Device) -> None  # noqa: F821\n",
    "        if self.cell_anchors is not None:\n",
    "            cell_anchors = self.cell_anchors\n",
    "            assert cell_anchors is not None\n",
    "            # suppose that all anchors have the same device\n",
    "            # which is a valid assumption in the current state of the codebase\n",
    "            if cell_anchors[0].device == device:\n",
    "                return\n",
    "\n",
    "        cell_anchors = [\n",
    "            self.generate_anchors(\n",
    "                sizes,\n",
    "                aspect_ratios,\n",
    "                dtype,\n",
    "                device\n",
    "            )\n",
    "            for sizes, aspect_ratios in zip(self.sizes, self.aspect_ratios)\n",
    "        ]\n",
    "        self.cell_anchors = cell_anchors\n",
    "\n",
    "    def num_anchors_per_location(self):\n",
    "        return [len(s) * len(a) for s, a in zip(self.sizes, self.aspect_ratios)]\n",
    "\n",
    "    # （self.cell_anchors、zip（grid_sizes、strides）、0：2）の（a、（g、s）、i）のすべての組み合わせについて、\n",
    "    # 方向iにs [i]だけ離れたg [i]アンカーを、aと同じ次元で出力\n",
    "    def grid_anchors(self, grid_sizes, strides):\n",
    "        # type: (List[List[int]], List[List[Tensor]]) -> List[Tensor]\n",
    "        anchors = []\n",
    "        cell_anchors = self.cell_anchors\n",
    "        assert cell_anchors is not None\n",
    "\n",
    "        for size, stride, base_anchors in zip(\n",
    "            grid_sizes, strides, cell_anchors\n",
    "        ):\n",
    "            grid_height, grid_width = size\n",
    "            stride_height, stride_width = stride\n",
    "            device = base_anchors.device\n",
    "\n",
    "            # For output anchor, compute [x_center, y_center, x_center, y_center]\n",
    "            shifts_x = torch.arange(\n",
    "                0, grid_width, dtype=torch.float32, device=device\n",
    "            ) * stride_width\n",
    "            shifts_y = torch.arange(\n",
    "                0, grid_height, dtype=torch.float32, device=device\n",
    "            ) * stride_height\n",
    "            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "            shift_x = shift_x.reshape(-1)\n",
    "            shift_y = shift_y.reshape(-1)\n",
    "            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)\n",
    "\n",
    "            # For every (base anchor, output anchor) pair,\n",
    "            # offset each zero-centered base anchor by the center of the output anchor.\n",
    "            anchors.append(\n",
    "                (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4)\n",
    "            )\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def cached_grid_anchors(self, grid_sizes, strides):\n",
    "        # type: (List[List[int]], List[List[Tensor]]) -> List[Tensor]\n",
    "        key = str(grid_sizes) + str(strides)\n",
    "        if key in self._cache:\n",
    "            return self._cache[key]\n",
    "        anchors = self.grid_anchors(grid_sizes, strides)\n",
    "        self._cache[key] = anchors\n",
    "        return anchors\n",
    "\n",
    "    def forward(self, image_list, feature_maps):\n",
    "        # type: (ImageList, List[Tensor]) -> List[Tensor]\n",
    "        grid_sizes = list([feature_map.shape[-2:] for feature_map in feature_maps])\n",
    "        image_size = image_list.tensors.shape[-2:]\n",
    "        dtype, device = feature_maps[0].dtype, feature_maps[0].device\n",
    "        strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n",
    "                    torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n",
    "        self.set_cell_anchors(dtype, device)\n",
    "        anchors_over_all_feature_maps = self.cached_grid_anchors(grid_sizes, strides)\n",
    "        anchors = torch.jit.annotate(List[List[torch.Tensor]], [])\n",
    "        for i, (image_height, image_width) in enumerate(image_list.image_sizes):\n",
    "            anchors_in_image = []\n",
    "            for anchors_per_feature_map in anchors_over_all_feature_maps:\n",
    "                anchors_in_image.append(anchors_per_feature_map)\n",
    "            anchors.append(anchors_in_image)\n",
    "        anchors = [torch.cat(anchors_per_image) for anchors_per_image in anchors]\n",
    "        # Clear the cache in case that memory leaks.\n",
    "        self._cache.clear()\n",
    "        return anchors\n",
    "\n",
    "\n",
    "class RPNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    クラス分類用と、矩形のズレ回帰モデルの作成\n",
    "    Arguments:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_anchors (int): number of anchors to be predicted\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_anchors):\n",
    "        super(RPNHead, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=1, stride=1)\n",
    "        self.bbox_pred = nn.Conv2d(\n",
    "            in_channels, num_anchors * 4, kernel_size=1, stride=1\n",
    "        )\n",
    "\n",
    "        for l in self.children():\n",
    "            torch.nn.init.normal_(l.weight, std=0.01)\n",
    "            torch.nn.init.constant_(l.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # type: (List[Tensor]) -> Tuple[List[Tensor], List[Tensor]]\n",
    "        logits = []\n",
    "        bbox_reg = []\n",
    "        for feature in x:\n",
    "            t = F.relu(self.conv(feature))\n",
    "            logits.append(self.cls_logits(t))\n",
    "            bbox_reg.append(self.bbox_pred(t))\n",
    "        return logits, bbox_reg\n",
    "\n",
    "\n",
    "def permute_and_flatten(layer, N, A, C, H, W):\n",
    "    # type: (Tensor, int, int, int, int, int) -> Tensor\n",
    "    layer = layer.view(N, -1, C, H, W)\n",
    "    layer = layer.permute(0, 3, 4, 1, 2)\n",
    "    layer = layer.reshape(N, -1, C)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def concat_box_prediction_layers(box_cls, box_regression):\n",
    "    # type: (List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
    "    box_cls_flattened = []\n",
    "    box_regression_flattened = []\n",
    "\n",
    "    #特徴ごとに、出力を並べ替えて、ラベルと同じ形式に変換\n",
    "    for box_cls_per_level, box_regression_per_level in zip(\n",
    "        box_cls, box_regression\n",
    "    ):\n",
    "        N, AxC, H, W = box_cls_per_level.shape\n",
    "        Ax4 = box_regression_per_level.shape[1]\n",
    "        A = Ax4 // 4\n",
    "        C = AxC // A\n",
    "        box_cls_per_level = permute_and_flatten(\n",
    "            box_cls_per_level, N, A, C, H, W\n",
    "        )\n",
    "        box_cls_flattened.append(box_cls_per_level)\n",
    "\n",
    "        box_regression_per_level = permute_and_flatten(\n",
    "            box_regression_per_level, N, A, 4, H, W\n",
    "        )\n",
    "        box_regression_flattened.append(box_regression_per_level)\n",
    "\n",
    "\n",
    "    box_cls = torch.cat(box_cls_flattened, dim=1).flatten(0, -2)\n",
    "    box_regression = torch.cat(box_regression_flattened, dim=1).reshape(-1, 4)\n",
    "    return box_cls, box_regression\n",
    "\n",
    "\n",
    "class RegionProposalNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Region Proposal Network (RPN).\n",
    "    Arguments:\n",
    "        anchor_generator (AnchorGenerator): アンカーの生成\n",
    "        head (nn.Module): ground truthとAnchorのズレの回帰問題と物体か背景かの分類問題を計算するモジュール\n",
    "        fg_iou_thresh (float): Anchorとground trouth BoxのIoU最小値 \n",
    "        bg_iou_thresh (float): Anchorとground trouth BoxのIoU最大値 \n",
    "        batch_size_per_image (int): 学習中にサンプリングされたAnchorの数\n",
    "        positive_fraction (float): 学習中のミニバッチ内の正のAnchorの値の割合\n",
    "        pre_nms_top_n (Dict[int]): Non-Maximum Suppressionをする前にある程度物体の候補を決めておく数\n",
    "        post_nms_top_n (Dict[int]): Non-Maximum Suppressionした後に物体の候補を決めておく数\n",
    "        nms_thresh (float):Non-Maximum Suppressionしきい値\n",
    "    \"\"\"\n",
    "    __annotations__ = {\n",
    "        'box_coder': det_utils.BoxCoder,\n",
    "        'proposal_matcher': det_utils.Matcher,\n",
    "        'fg_bg_sampler': det_utils.BalancedPositiveNegativeSampler,\n",
    "        'pre_nms_top_n': Dict[str, int],\n",
    "        'post_nms_top_n': Dict[str, int],\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 anchor_generator,\n",
    "                 head,\n",
    "                 #\n",
    "                 fg_iou_thresh, bg_iou_thresh,\n",
    "                 batch_size_per_image, positive_fraction,\n",
    "                 #\n",
    "                 pre_nms_top_n, post_nms_top_n, nms_thresh):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.head = head\n",
    "        self.box_coder = det_utils.BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))\n",
    "\n",
    "        # used during training\n",
    "        self.box_similarity = box_ops.box_iou\n",
    "\n",
    "        self.proposal_matcher = det_utils.Matcher(\n",
    "            fg_iou_thresh,\n",
    "            bg_iou_thresh,\n",
    "            allow_low_quality_matches=True,\n",
    "        )\n",
    "\n",
    "        self.fg_bg_sampler = det_utils.BalancedPositiveNegativeSampler(\n",
    "            batch_size_per_image, positive_fraction\n",
    "        )\n",
    "        # used during testing\n",
    "        self._pre_nms_top_n = pre_nms_top_n\n",
    "        self._post_nms_top_n = post_nms_top_n\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.min_size = 1e-3\n",
    "\n",
    "    def pre_nms_top_n(self):\n",
    "        if self.training:\n",
    "            return self._pre_nms_top_n['training']\n",
    "        return self._pre_nms_top_n['testing']\n",
    "\n",
    "    def post_nms_top_n(self):\n",
    "        if self.training:\n",
    "            return self._post_nms_top_n['training']\n",
    "        return self._post_nms_top_n['testing']\n",
    "\n",
    "    def assign_targets_to_anchors(self, anchors, targets):\n",
    "        # type: (List[Tensor], List[Dict[str, Tensor]]) -> Tuple[List[Tensor], List[Tensor]]\n",
    "        labels = []\n",
    "        matched_gt_boxes = []\n",
    "        for anchors_per_image, targets_per_image in zip(anchors, targets):\n",
    "            gt_boxes = targets_per_image[\"boxes\"]\n",
    "\n",
    "            if gt_boxes.numel() == 0:\n",
    "                # Background image (negative example)\n",
    "                device = anchors_per_image.device\n",
    "                matched_gt_boxes_per_image = torch.zeros(anchors_per_image.shape, dtype=torch.float32, device=device)\n",
    "                labels_per_image = torch.zeros((anchors_per_image.shape[0],), dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                match_quality_matrix = box_ops.box_iou(gt_boxes, anchors_per_image)\n",
    "                matched_idxs = self.proposal_matcher(match_quality_matrix)\n",
    "                # get the targets corresponding GT for each proposal\n",
    "                # NB: need to clamp the indices because we can have a single\n",
    "                # GT in the image, and matched_idxs can be -2, which goes\n",
    "                # out of bounds\n",
    "                matched_gt_boxes_per_image = gt_boxes[matched_idxs.clamp(min=0)]\n",
    "\n",
    "                labels_per_image = matched_idxs >= 0\n",
    "                labels_per_image = labels_per_image.to(dtype=torch.float32)\n",
    "\n",
    "                # Background (negative examples)\n",
    "                bg_indices = matched_idxs == self.proposal_matcher.BELOW_LOW_THRESHOLD\n",
    "                labels_per_image[bg_indices] = torch.tensor(0.0)\n",
    "\n",
    "                # discard indices that are between thresholds\n",
    "                inds_to_discard = matched_idxs == self.proposal_matcher.BETWEEN_THRESHOLDS\n",
    "                labels_per_image[inds_to_discard] = torch.tensor(-1.0)\n",
    "\n",
    "            labels.append(labels_per_image)\n",
    "            matched_gt_boxes.append(matched_gt_boxes_per_image)\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    def _get_top_n_idx(self, objectness, num_anchors_per_level):\n",
    "        # type: (Tensor, List[int]) -> Tensor\n",
    "        r = []\n",
    "        offset = 0\n",
    "        for ob in objectness.split(num_anchors_per_level, 1):\n",
    "            if torchvision._is_tracing():\n",
    "                num_anchors, pre_nms_top_n = _onnx_get_num_anchors_and_pre_nms_top_n(ob, self.pre_nms_top_n())\n",
    "            else:\n",
    "                num_anchors = ob.shape[1]\n",
    "                pre_nms_top_n = min(self.pre_nms_top_n(), num_anchors)\n",
    "            _, top_n_idx = ob.topk(pre_nms_top_n, dim=1)\n",
    "            r.append(top_n_idx + offset)\n",
    "            offset += num_anchors\n",
    "        return torch.cat(r, dim=1)\n",
    "\n",
    "    def filter_proposals(self, proposals, objectness, image_shapes, num_anchors_per_level):\n",
    "        # type: (Tensor, Tensor, List[Tuple[int, int]], List[int]) -> Tuple[List[Tensor], List[Tensor]]\n",
    "        num_images = proposals.shape[0]\n",
    "        device = proposals.device\n",
    "        # do not backprop throught objectness\n",
    "        objectness = objectness.detach()\n",
    "        objectness = objectness.reshape(num_images, -1)\n",
    "\n",
    "        levels = [\n",
    "            torch.full((n,), idx, dtype=torch.int64, device=device)\n",
    "            for idx, n in enumerate(num_anchors_per_level)\n",
    "        ]\n",
    "        levels = torch.cat(levels, 0)\n",
    "        levels = levels.reshape(1, -1).expand_as(objectness)\n",
    "\n",
    "        # select top_n boxes independently per level before applying nms\n",
    "        top_n_idx = self._get_top_n_idx(objectness, num_anchors_per_level)\n",
    "\n",
    "        image_range = torch.arange(num_images, device=device)\n",
    "        batch_idx = image_range[:, None]\n",
    "\n",
    "        objectness = objectness[batch_idx, top_n_idx]\n",
    "        levels = levels[batch_idx, top_n_idx]\n",
    "        proposals = proposals[batch_idx, top_n_idx]\n",
    "\n",
    "        final_boxes = []\n",
    "        final_scores = []\n",
    "        for boxes, scores, lvl, img_shape in zip(proposals, objectness, levels, image_shapes):\n",
    "            boxes = box_ops.clip_boxes_to_image(boxes, img_shape)\n",
    "            keep = box_ops.remove_small_boxes(boxes, self.min_size)\n",
    "            boxes, scores, lvl = boxes[keep], scores[keep], lvl[keep]\n",
    "            # non-maximum suppression, independently done per level\n",
    "            keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)\n",
    "            # keep only topk scoring predictions\n",
    "            keep = keep[:self.post_nms_top_n()]\n",
    "            boxes, scores = boxes[keep], scores[keep]\n",
    "            final_boxes.append(boxes)\n",
    "            final_scores.append(scores)\n",
    "        return final_boxes, final_scores\n",
    "\n",
    "    def compute_loss(self, objectness, pred_bbox_deltas, labels, regression_targets):\n",
    "        # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n",
    "\n",
    "        sampled_pos_inds, sampled_neg_inds = self.fg_bg_sampler(labels)\n",
    "        sampled_pos_inds = torch.nonzero(torch.cat(sampled_pos_inds, dim=0)).squeeze(1)\n",
    "        sampled_neg_inds = torch.nonzero(torch.cat(sampled_neg_inds, dim=0)).squeeze(1)\n",
    "\n",
    "        sampled_inds = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)\n",
    "\n",
    "        objectness = objectness.flatten()\n",
    "\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        regression_targets = torch.cat(regression_targets, dim=0)\n",
    "        \n",
    "        #ground truthとあるAnchor boxのズレのlossをL1ノルムで計算\n",
    "        box_loss = det_utils.smooth_l1_loss(\n",
    "            pred_bbox_deltas[sampled_pos_inds],\n",
    "            regression_targets[sampled_pos_inds],\n",
    "            beta=1 / 9,\n",
    "            size_average=False,\n",
    "        ) / (sampled_inds.numel())\n",
    "        \n",
    "        #ground truthとAnchor boxesのIOUで背景か物体かのlossを\n",
    "        #バイナリクロスエントロピーで計算する\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "            objectness[sampled_inds], labels[sampled_inds]\n",
    "        )\n",
    "\n",
    "        return objectness_loss, box_loss\n",
    "\n",
    "    def forward(self,\n",
    "                images,       # type: ImageList\n",
    "                features,     # type: Dict[str, Tensor]\n",
    "                targets=None  # type: Optional[List[Dict[str, Tensor]]]\n",
    "                ):\n",
    "        # type: (...) -> Tuple[List[Tensor], Dict[str, Tensor]]\n",
    "        \"\"\"\n",
    "        引数:\n",
    "            images (ImageList): 画像\n",
    "            features (OrderedDict[Tensor]): 画像の特徴量\n",
    "            targets (List[Dict[Tensor]]): 画像に存在するground-truth boxes\n",
    "        Returns:\n",
    "            boxes (List[Tensor]): RPNから予測されたbox,画像1枚当たり1テンソル\n",
    "            \n",
    "            losses (Dict[Tensor]): 学習のloss\n",
    "        \"\"\"\n",
    "        # RPN uses all feature maps that are available\n",
    "        features = list(features.values())\n",
    "        objectness, pred_bbox_deltas = self.head(features)\n",
    "        anchors = self.anchor_generator(images, features)\n",
    "\n",
    "        num_images = len(anchors)\n",
    "        num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
    "        num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
    "        objectness, pred_bbox_deltas = \\\n",
    "            concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
    "        # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
    "        # note that we detach the deltas because Faster R-CNN do not backprop through\n",
    "        # the proposals\n",
    "        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
    "        proposals = proposals.view(num_images, -1, 4)\n",
    "        boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
    "\n",
    "        losses = {}\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)\n",
    "            regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)\n",
    "            loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
    "                objectness, pred_bbox_deltas, labels, regression_targets)\n",
    "            losses = {\n",
    "                \"loss_objectness\": loss_objectness,\n",
    "                \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
    "            }\n",
    "        return boxes, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FasterRCNNのプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "from ..utils import load_state_dict_from_url\n",
    "\n",
    "# from .generalized_rcnn import GeneralizedRCNN\n",
    "from .rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "from .roi_heads import RoIHeads\n",
    "from .transform import GeneralizedRCNNTransform\n",
    "from .backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"FasterRCNN\", \"fasterrcnn_resnet50_fpn\",\n",
    "]\n",
    "\n",
    "\n",
    "class FasterRCNN(GeneralizedRCNN):\n",
    "\n",
    "    def __init__(self, backbone, num_classes=None,\n",
    "                 # transform parameters\n",
    "                 min_size=800, max_size=1333,\n",
    "                 image_mean=None, image_std=None,\n",
    "                 \n",
    "                 # RPN parameters\n",
    "                 \"\"\" 上のRPNで説明済み\"\"\"\n",
    "                 rpn_anchor_generator=None, rpn_head=None,\n",
    "                 rpn_pre_nms_top_n_train=2000, rpn_pre_nms_top_n_test=1000,\n",
    "                 rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=1000,\n",
    "                 rpn_nms_thresh=0.7,\n",
    "                 rpn_fg_iou_thresh=0.7, rpn_bg_iou_thresh=0.3,\n",
    "                 rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n",
    "                 \n",
    "    \"\"\"\n",
    "    box_roi_pool（MultiScaleRoIAlign）：特徴マップ切り取り、サイズを変更するモジュール\n",
    "    box_head（nn.Module）：トリミングされた特徴マップを入力として受け取るモジュール\n",
    "    box_predictor（nn.Module）：box_headの出力を受け取り、物体かの分類とボックス回帰デルタを返すモジュール。\n",
    "    box_score_thresh（float）：box_score_threshより大きい分類スコアを持つ提案のみを返します\n",
    "    box_nms_thresh（float）：予測ヘッドのNMSしきい値。\n",
    "    box_detections_per_img（int）：すべてのクラスの画像あたりの最大検出数。\n",
    "    box_batch_size_per_image（int）：物体分類のトレーニング中にサンプリングされたboxの数\n",
    "    box_positive_fraction（float）：分類計算モジュールのトレーニング中のミニバッチでの正の値の割合              \n",
    "    bbox_reg_weights（Tuple [float、float、float、float]）：バウンディングボックスのエンコード/デコードの重み    \n",
    "    \"\"\"\n",
    "                 # Box parameters\n",
    "                 box_roi_pool=None, box_head=None, box_predictor=None,\n",
    "                 box_score_thresh=0.05, box_nms_thresh=0.5, box_detections_per_img=100,\n",
    "                 box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n",
    "                 box_batch_size_per_image=512, box_positive_fraction=0.25,\n",
    "                 bbox_reg_weights=None):\n",
    "\n",
    "        if not hasattr(backbone, \"out_channels\"):\n",
    "            raise ValueError(\n",
    "                \"backbone should contain an attribute out_channels \"\n",
    "                \"specifying the number of output channels (assumed to be the \"\n",
    "                \"same for all the levels)\")\n",
    "\n",
    "        assert isinstance(rpn_anchor_generator, (AnchorGenerator, type(None)))\n",
    "        assert isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None)))\n",
    "\n",
    "        if num_classes is not None:\n",
    "            if box_predictor is not None:\n",
    "                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n",
    "        else:\n",
    "            if box_predictor is None:\n",
    "                raise ValueError(\"num_classes should not be None when box_predictor \"\n",
    "                                 \"is not specified\")\n",
    "\n",
    "        out_channels = backbone.out_channels\n",
    "\n",
    "        if rpn_anchor_generator is None:\n",
    "            anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "            aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "            rpn_anchor_generator = AnchorGenerator(\n",
    "                anchor_sizes, aspect_ratios\n",
    "            )\n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(\n",
    "                out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    "            )\n",
    "\n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "        \n",
    "        #RPNの生成\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator, rpn_head,\n",
    "            rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)\n",
    "\n",
    "        if box_roi_pool is None:\n",
    "            box_roi_pool = MultiScaleRoIAlign(\n",
    "                featmap_names=['0', '1', '2', '3'],\n",
    "                output_size=7,\n",
    "                sampling_ratio=2)\n",
    "\n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            representation_size = 1024\n",
    "            box_head = TwoMLPHead(\n",
    "                out_channels * resolution ** 2,\n",
    "                representation_size)\n",
    "\n",
    "        if box_predictor is None:\n",
    "            representation_size = 1024\n",
    "            box_predictor = FastRCNNPredictor(\n",
    "                representation_size,\n",
    "                num_classes)\n",
    "\n",
    "        roi_heads = RoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool, box_head, box_predictor,\n",
    "            box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "            box_batch_size_per_image, box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh, box_nms_thresh, box_detections_per_img)\n",
    "\n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n",
    "\n",
    "        super(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)\n",
    "\n",
    "\n",
    "class TwoMLPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    FPN-based(Feature Pyramid Net) modelsのクラス\n",
    "    Arguments:\n",
    "        in_channels (int): number of input channels\n",
    "        representation_size (int): size of the intermediate representation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, representation_size):\n",
    "        super(TwoMLPHead, self).__init__()\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels, representation_size)\n",
    "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FastRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast R-CNNの標準分類+境界ボックス回帰層\n",
    "    Arguments:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictor, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'fasterrcnn_resnet50_fpn_coco':\n",
    "        'https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def fasterrcnn_resnet50_fpn(pretrained=False, progress=True,\n",
    "                            num_classes=91, pretrained_backbone=True, **kwargs):\n",
    "    \"\"\"\n",
    "    ResNet-50-FPNがバックボーンでFaster R-CNNモデルを構築している。\n",
    "   \n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    backbone = resnet_fpn_backbone('resnet50', pretrained_backbone)\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['fasterrcnn_resnet50_fpn_coco'],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\honbu.DESKTOP-RCLGRQ4/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "precision = 'fp32'\n",
    "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD300(\n",
       "  (feature_extractor): ResNet(\n",
       "    (feature_extractor): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (additional_blocks): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (loc): ModuleList(\n",
       "    (0): Conv2d(1024, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conf): ModuleList(\n",
       "    (0): Conv2d(1024, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(512, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Conv2d(256, 486, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): Conv2d(256, 324, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"参項文献：https://github.com/YutaroOgawa/pytorch_advanced\"\"\"\n",
    "\n",
    "\n",
    "# パッケージのimport\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from itertools import product as product\n",
    "from math import sqrt as sqrt\n",
    "\n",
    "# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# フォルダ「utils」のdata_augumentation.pyからimport。入力画像の前処理をするクラス\n",
    "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
    "\n",
    "# フォルダ「utils」にある関数matchを記述したmatch.pyからimport\n",
    "from utils.match import match\n",
    "\n",
    "\n",
    "# 学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する\n",
    "\n",
    "def make_datapath_list(rootpath):\n",
    "    \"\"\"\n",
    "    データへのパスを格納したリストを作成する。\n",
    "    Parameters\n",
    "    ----------\n",
    "    rootpath : str\n",
    "        データフォルダへのパス\n",
    "    Returns\n",
    "    -------\n",
    "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
    "        データへのパスを格納したリスト\n",
    "    \"\"\"\n",
    "\n",
    "    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n",
    "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
    "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
    "\n",
    "    # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する\n",
    "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
    "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
    "\n",
    "    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "\n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()  # 空白スペースと改行を除去\n",
    "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
    "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
    "        train_img_list.append(img_path)  # リストに追加\n",
    "        train_anno_list.append(anno_path)  # リストに追加\n",
    "\n",
    "    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "\n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()  # 空白スペースと改行を除去\n",
    "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
    "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
    "        val_img_list.append(img_path)  # リストに追加\n",
    "        val_anno_list.append(anno_path)  # リストに追加\n",
    "\n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list\n",
    "\n",
    "\n",
    "# 「XML形式のアノテーション」を、リスト形式に変換するクラス\n",
    "\n",
    "\n",
    "class Anno_xml2list(object):\n",
    "    \"\"\"\n",
    "    1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes : リスト\n",
    "        VOCのクラス名を格納したリスト\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classes):\n",
    "\n",
    "        self.classes = classes\n",
    "\n",
    "    def __call__(self, xml_path, width, height):\n",
    "        \"\"\"\n",
    "        1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        xml_path : str\n",
    "            xmlファイルへのパス。\n",
    "        width : int\n",
    "            対象画像の幅。\n",
    "        height : int\n",
    "            対象画像の高さ。\n",
    "        Returns\n",
    "        -------\n",
    "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
    "            物体のアノテーションデータを格納したリスト。画像内に存在する物体数分のだけ要素を持つ。\n",
    "        \"\"\"\n",
    "\n",
    "        # 画像内の全ての物体のアノテーションをこのリストに格納します\n",
    "        ret = []\n",
    "\n",
    "        # xmlファイルを読み込む\n",
    "        xml = ET.parse(xml_path).getroot()\n",
    "\n",
    "        # 画像内にある物体（object）の数だけループする\n",
    "        for obj in xml.iter('object'):\n",
    "\n",
    "            # アノテーションで検知がdifficultに設定されているものは除外\n",
    "            difficult = int(obj.find('difficult').text)\n",
    "            if difficult == 1:\n",
    "                continue\n",
    "\n",
    "            # 1つの物体に対するアノテーションを格納するリスト\n",
    "            bndbox = []\n",
    "\n",
    "            name = obj.find('name').text.lower().strip()  # 物体名\n",
    "            bbox = obj.find('bndbox')  # バウンディングボックスの情報\n",
    "\n",
    "            # アノテーションの xmin, ymin, xmax, ymaxを取得し、0～1に規格化\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "\n",
    "            for pt in (pts):\n",
    "                # VOCは原点が(1,1)なので1を引き算して（0, 0）に\n",
    "                cur_pixel = int(bbox.find(pt).text) - 1\n",
    "\n",
    "                # 幅、高さで規格化\n",
    "                if pt == 'xmin' or pt == 'xmax':  # x方向のときは幅で割算\n",
    "                    cur_pixel /= width\n",
    "                else:  # y方向のときは高さで割算\n",
    "                    cur_pixel /= height\n",
    "\n",
    "                bndbox.append(cur_pixel)\n",
    "\n",
    "            # アノテーションのクラス名のindexを取得して追加\n",
    "            label_idx = self.classes.index(name)\n",
    "            bndbox.append(label_idx)\n",
    "\n",
    "            # resに[xmin, ymin, xmax, ymax, label_ind]を足す\n",
    "            ret += [bndbox]\n",
    "\n",
    "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
    "\n",
    "\n",
    "# 入力画像の前処理をするクラス\n",
    "\n",
    "\n",
    "class DataTransform():\n",
    "    \"\"\"\n",
    "    画像とアノテーションの前処理クラス。訓練と推論で異なる動作をする。\n",
    "    画像のサイズを300x300にする。\n",
    "    学習時はデータオーギュメンテーションする。\n",
    "    Attributes\n",
    "    ----------\n",
    "    input_size : int\n",
    "        リサイズ先の画像の大きさ。\n",
    "    color_mean : (B, G, R)\n",
    "        各色チャネルの平均値。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, color_mean):\n",
    "        self.data_transform = {\n",
    "            'train': Compose([\n",
    "                ConvertFromInts(),  # intをfloat32に変換\n",
    "                ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n",
    "                PhotometricDistort(),  # 画像の色調などをランダムに変化\n",
    "                Expand(color_mean),  # 画像のキャンバスを広げる\n",
    "                RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n",
    "                RandomMirror(),  # 画像を反転させる\n",
    "                ToPercentCoords(),  # アノテーションデータを0-1に規格化\n",
    "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
    "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                ConvertFromInts(),  # intをfloatに変換\n",
    "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
    "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, phase, boxes, labels):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        phase : 'train' or 'val'\n",
    "            前処理のモードを指定。\n",
    "        \"\"\"\n",
    "        return self.data_transform[phase](img, boxes, labels)\n",
    "\n",
    "\n",
    "class VOCDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
    "    Attributes\n",
    "    ----------\n",
    "    img_list : リスト\n",
    "        画像のパスを格納したリスト\n",
    "    anno_list : リスト\n",
    "        アノテーションへのパスを格納したリスト\n",
    "    phase : 'train' or 'test'\n",
    "        学習か訓練かを設定する。\n",
    "    transform : object\n",
    "        前処理クラスのインスタンス\n",
    "    transform_anno : object\n",
    "        xmlのアノテーションをリストに変換するインスタンス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase  # train もしくは valを指定\n",
    "        self.transform = transform  # 画像の変形\n",
    "        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n",
    "\n",
    "    def __len__(self):\n",
    "        '''画像の枚数を返す'''\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        前処理をした画像のテンソル形式のデータとアノテーションを取得\n",
    "        '''\n",
    "        im, gt, h, w = self.pull_item(index)\n",
    "        return im, gt\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n",
    "\n",
    "        # 1. 画像読み込み\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
    "        height, width, channels = img.shape  # 画像のサイズを取得\n",
    "\n",
    "        # 2. xml形式のアノテーション情報をリストに\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
    "\n",
    "        # 3. 前処理を実施\n",
    "        img, boxes, labels = self.transform(\n",
    "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
    "\n",
    "        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n",
    "        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n",
    "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
    "\n",
    "        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n",
    "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "\n",
    "        return img, gt, height, width\n",
    "\n",
    "\n",
    "def od_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。\n",
    "    画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
    "    この変化に対応したDataLoaderを作成するために、\n",
    "    カスタイマイズした、collate_fnを作成します。\n",
    "    collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
    "    ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
    "    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
    "    \"\"\"\n",
    "\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])  # sample[0] は画像imgです\n",
    "        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n",
    "\n",
    "    # imgsはミニバッチサイズのリストになっています\n",
    "    # リストの要素はtorch.Size([3, 300, 300])です。\n",
    "    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "    # targetsはアノテーションデータの正解であるgtのリストです。\n",
    "    # リストのサイズはミニバッチサイズです。\n",
    "    # リストtargetsの要素は [n, 5] となっています。\n",
    "    # nは画像ごとに異なり、画像内にある物体の数となります。\n",
    "    # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
    "\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "# 34層にわたる、vggモジュールを作成\n",
    "def make_vgg():\n",
    "    layers = []\n",
    "    in_channels = 3  # 色チャネル数\n",
    "\n",
    "    # vggモジュールで使用する畳み込み層やマックスプーリングのチャネル数\n",
    "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\n",
    "           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n",
    "\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'MC':\n",
    "            # ceilは出力サイズを、計算結果（float）に対して、切り上げで整数にするモード\n",
    "            # デフォルトでは出力サイズを計算結果（float）に対して、切り下げで整数にするfloorモード\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "# 8層にわたる、extrasモジュールを作成\n",
    "def make_extras():\n",
    "    layers = []\n",
    "    in_channels = 1024  # vggモジュールから出力された、extraに入力される画像チャネル数\n",
    "\n",
    "    # extraモジュールの畳み込み層のチャネル数を設定するコンフィギュレーション\n",
    "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n",
    "\n",
    "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n",
    "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n",
    "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n",
    "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n",
    "\n",
    "    return nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "# デフォルトボックスのオフセットを出力するloc_layers、\n",
    "# デフォルトボックスに対する各クラスの確率を出力するconf_layersを作成\n",
    "\n",
    "\n",
    "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
    "\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "\n",
    "    # VGGの22層目、conv4_3（source1）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # VGGの最終層（source2）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source3）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source4）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source5）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    # extraの（source6）に対する畳み込み層\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
    "                             * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
    "                              * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n",
    "\n",
    "\n",
    "# convC4_3からの出力をscale=20のL2Normで正規化する層\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, input_channels=512, scale=20):\n",
    "        super(L2Norm, self).__init__()  # 親クラスのコンストラクタ実行\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
    "        self.scale = scale  # 係数weightの初期値として設定する値\n",
    "        self.reset_parameters()  # パラメータの初期化\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        '''結合パラメータを大きさscaleの値にする初期化を実行'''\n",
    "        init.constant_(self.weight, self.scale)  # weightの値がすべてscale（=20）になる\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''38×38の特徴量に対して、512チャネルにわたって2乗和のルートを求めた\n",
    "        38×38個の値を使用し、各特徴量を正規化してから係数をかけ算する層'''\n",
    "\n",
    "        # 各チャネルにおける38×38個の特徴量のチャネル方向の2乗和を計算し、\n",
    "        # さらにルートを求め、割り算して正規化する\n",
    "        # normのテンソルサイズはtorch.Size([batch_num, 1, 38, 38])になります\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
    "        x = torch.div(x, norm)\n",
    "\n",
    "        # 係数をかける。係数はチャネルごとに1つで、512個の係数を持つ\n",
    "        # self.weightのテンソルサイズはtorch.Size([512])なので\n",
    "        # torch.Size([batch_num, 512, 38, 38])まで変形します\n",
    "        weights = self.weight.unsqueeze(\n",
    "            0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        out = weights * x\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# デフォルトボックスを出力するクラス\n",
    "class DBox(object):\n",
    "    def __init__(self, cfg):\n",
    "        super(DBox, self).__init__()\n",
    "\n",
    "        # 初期設定\n",
    "        self.image_size = cfg['input_size']  # 画像サイズの300\n",
    "        # [38, 19, …] 各sourceの特徴量マップのサイズ\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.num_priors = len(cfg[\"feature_maps\"])  # sourceの個数=6\n",
    "        self.steps = cfg['steps']  # [8, 16, …] DBoxのピクセルサイズ\n",
    "\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        # [30, 60, …] 小さい正方形のDBoxのピクセルサイズ(正確には面積)\n",
    "\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        # [60, 111, …] 大きい正方形のDBoxのピクセルサイズ(正確には面積)\n",
    "\n",
    "        self.aspect_ratios = cfg['aspect_ratios']  # 長方形のDBoxのアスペクト比\n",
    "\n",
    "    def make_dbox_list(self):\n",
    "        '''DBoxを作成する'''\n",
    "        mean = []\n",
    "        # 'feature_maps': [38, 19, 10, 5, 3, 1]\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):  # fまでの数で2ペアの組み合わせを作る　f_P_2 個\n",
    "                # 特徴量の画像サイズ\n",
    "                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "\n",
    "                # DBoxの中心座標 x,y　ただし、0～1で規格化している\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                # アスペクト比1の小さいDBox [cx,cy, width, height]\n",
    "                # 'min_sizes': [30, 60, 111, 162, 213, 264]\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # アスペクト比1の大きいDBox [cx,cy, width, height]\n",
    "                # 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # その他のアスペクト比のdefBox [cx,cy, width, height]\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "\n",
    "        # DBoxをテンソルに変換 torch.Size([8732, 4])\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "\n",
    "        # DBoxの大きさが1を超えている場合は1にする\n",
    "        output.clamp_(max=1, min=0)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# オフセット情報を使い、DBoxをBBoxに変換する関数\n",
    "def decode(loc, dbox_list):\n",
    "    \"\"\"\n",
    "    オフセット情報を使い、DBoxをBBoxに変換する。\n",
    "    Parameters\n",
    "    ----------\n",
    "    loc:  [8732,4]\n",
    "        SSDモデルで推論するオフセット情報。\n",
    "    dbox_list: [8732,4]\n",
    "        DBoxの情報\n",
    "    Returns\n",
    "    -------\n",
    "    boxes : [xmin, ymin, xmax, ymax]\n",
    "        BBoxの情報\n",
    "    \"\"\"\n",
    "\n",
    "    # DBoxは[cx, cy, width, height]で格納されている\n",
    "    # locも[Δcx, Δcy, Δwidth, Δheight]で格納されている\n",
    "\n",
    "    # オフセット情報からBBoxを求める\n",
    "    boxes = torch.cat((\n",
    "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n",
    "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n",
    "    # boxesのサイズはtorch.Size([8732, 4])となります\n",
    "\n",
    "    # BBoxの座標情報を[cx, cy, width, height]から[xmin, ymin, xmax, ymax] に\n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2  # 座標(xmin,ymin)へ変換\n",
    "    boxes[:, 2:] += boxes[:, :2]  # 座標(xmax,ymax)へ変換\n",
    "\n",
    "    return boxes\n",
    "\n",
    "# Non-Maximum Suppressionを行う関数\n",
    "\n",
    "\n",
    "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppressionを行う関数。\n",
    "    boxesのうち被り過ぎ（overlap以上）のBBoxを削除する。\n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes : [確信度閾値（0.01）を超えたBBox数,4]\n",
    "        BBox情報。\n",
    "    scores :[確信度閾値（0.01）を超えたBBox数]\n",
    "        confの情報\n",
    "    Returns\n",
    "    -------\n",
    "    keep : リスト\n",
    "        confの降順にnmsを通過したindexが格納\n",
    "    count：int\n",
    "        nmsを通過したBBoxの数\n",
    "    \"\"\"\n",
    "\n",
    "    # returnのひな形を作成\n",
    "    count = 0\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    # keep：torch.Size([確信度閾値を超えたBBox数])、要素は全部0\n",
    "\n",
    "    # 各BBoxの面積areaを計算\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "\n",
    "    # boxesをコピーする。後で、BBoxの被り度合いIOUの計算に使用する際のひな形として用意\n",
    "    tmp_x1 = boxes.new()\n",
    "    tmp_y1 = boxes.new()\n",
    "    tmp_x2 = boxes.new()\n",
    "    tmp_y2 = boxes.new()\n",
    "    tmp_w = boxes.new()\n",
    "    tmp_h = boxes.new()\n",
    "\n",
    "    # socreを昇順に並び変える\n",
    "    v, idx = scores.sort(0)\n",
    "\n",
    "    # 上位top_k個（200個）のBBoxのindexを取り出す（200個存在しない場合もある）\n",
    "    idx = idx[-top_k:]\n",
    "\n",
    "    # idxの要素数が0でない限りループする\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # 現在のconf最大のindexをiに\n",
    "\n",
    "        # keepの現在の最後にconf最大のindexを格納する\n",
    "        # このindexのBBoxと被りが大きいBBoxをこれから消去する\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "\n",
    "        # 最後のBBoxになった場合は、ループを抜ける\n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "\n",
    "        # 現在のconf最大のindexをkeepに格納したので、idxをひとつ減らす\n",
    "        idx = idx[:-1]\n",
    "\n",
    "        # -------------------\n",
    "        # これからkeepに格納したBBoxと被りの大きいBBoxを抽出して除去する\n",
    "        # -------------------\n",
    "        # ひとつ減らしたidxまでのBBoxを、outに指定した変数として作成する\n",
    "        torch.index_select(x1, 0, idx, out=tmp_x1)\n",
    "        torch.index_select(y1, 0, idx, out=tmp_y1)\n",
    "        torch.index_select(x2, 0, idx, out=tmp_x2)\n",
    "        torch.index_select(y2, 0, idx, out=tmp_y2)\n",
    "\n",
    "        # すべてのBBoxに対して、現在のBBox=indexがiと被っている値までに設定(clamp)\n",
    "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n",
    "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
    "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
    "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n",
    "\n",
    "        # wとhのテンソルサイズをindexを1つ減らしたものにする\n",
    "        tmp_w.resize_as_(tmp_x2)\n",
    "        tmp_h.resize_as_(tmp_y2)\n",
    "\n",
    "        # clampした状態でのBBoxの幅と高さを求める\n",
    "        tmp_w = tmp_x2 - tmp_x1\n",
    "        tmp_h = tmp_y2 - tmp_y1\n",
    "\n",
    "        # 幅や高さが負になっているものは0にする\n",
    "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
    "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
    "\n",
    "        # clampされた状態での面積を求める\n",
    "        inter = tmp_w*tmp_h\n",
    "\n",
    "        # IoU = intersect部分 / (area(a) + area(b) - intersect部分)の計算\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # 各BBoxの元の面積\n",
    "        union = (rem_areas - inter) + area[i]  # 2つのエリアのANDの面積\n",
    "        IoU = inter/union\n",
    "\n",
    "        # IoUがoverlapより小さいidxのみを残す\n",
    "        idx = idx[IoU.le(overlap)]  # leはLess than or Equal toの処理をする演算です\n",
    "        # IoUがoverlapより大きいidxは、最初に選んでkeepに格納したidxと同じ物体に対してBBoxを囲んでいるため消去\n",
    "\n",
    "    # whileのループが抜けたら終了\n",
    "\n",
    "    return keep, count\n",
    "\n",
    "\n",
    "# SSDの推論時にconfとlocの出力から、被りを除去したBBoxを出力する\n",
    "\n",
    "\n",
    "class Detect(Function):\n",
    "\n",
    "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\n",
    "        self.softmax = nn.Softmax(dim=-1)  # confをソフトマックス関数で正規化するために用意\n",
    "        self.conf_thresh = conf_thresh  # confがconf_thresh=0.01より高いDBoxのみを扱う\n",
    "        self.top_k = top_k  # nm_supressionでconfの高いtop_k個を計算に使用する, top_k = 200\n",
    "        self.nms_thresh = nms_thresh  # nm_supressionでIOUがnms_thresh=0.45より大きいと、同一物体へのBBoxとみなす\n",
    "\n",
    "    def forward(self, loc_data, conf_data, dbox_list):\n",
    "        \"\"\"\n",
    "        順伝搬の計算を実行する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        loc_data:  [batch_num,8732,4]\n",
    "            オフセット情報。\n",
    "        conf_data: [batch_num, 8732,num_classes]\n",
    "            検出の確信度。\n",
    "        dbox_list: [8732,4]\n",
    "            DBoxの情報\n",
    "        Returns\n",
    "        -------\n",
    "        output : torch.Size([batch_num, 21, 200, 5])\n",
    "            （batch_num、クラス、confのtop200、BBoxの情報）\n",
    "        \"\"\"\n",
    "\n",
    "        # 各サイズを取得\n",
    "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
    "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
    "        num_classes = conf_data.size(2)  # クラス数 = 21\n",
    "\n",
    "        # confはソフトマックスを適用して正規化する\n",
    "        conf_data = self.softmax(conf_data)\n",
    "\n",
    "        # 出力の型を作成する。テンソルサイズは[minibatch数, 21, 200, 5]\n",
    "        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n",
    "\n",
    "        # cof_dataを[batch_num,8732,num_classes]から[batch_num, num_classes,8732]に順番変更\n",
    "        conf_preds = conf_data.transpose(2, 1)\n",
    "\n",
    "        # ミニバッチごとのループ\n",
    "        for i in range(num_batch):\n",
    "\n",
    "            # 1. locとDBoxから修正したBBox [xmin, ymin, xmax, ymax] を求める\n",
    "            decoded_boxes = decode(loc_data[i], dbox_list)\n",
    "\n",
    "            # confのコピーを作成\n",
    "            conf_scores = conf_preds[i].clone()\n",
    "\n",
    "            # 画像クラスごとのループ（背景クラスのindexである0は計算せず、index=1から）\n",
    "            for cl in range(1, num_classes):\n",
    "\n",
    "                # 2.confの閾値を超えたBBoxを取り出す\n",
    "                # confの閾値を超えているかのマスクを作成し、\n",
    "                # 閾値を超えたconfのインデックスをc_maskとして取得\n",
    "                c_mask = conf_scores[cl].gt(self.conf_thresh)\n",
    "                # gtはGreater thanのこと。gtにより閾値を超えたものが1に、以下が0になる\n",
    "                # conf_scores:torch.Size([21, 8732])\n",
    "                # c_mask:torch.Size([8732])\n",
    "\n",
    "                # scoresはtorch.Size([閾値を超えたBBox数])\n",
    "                scores = conf_scores[cl][c_mask]\n",
    "\n",
    "                # 閾値を超えたconfがない場合、つまりscores=[]のときは、何もしない\n",
    "                if scores.nelement() == 0:  # nelementで要素数の合計を求める\n",
    "                    continue\n",
    "\n",
    "                # c_maskを、decoded_boxesに適用できるようにサイズを変更します\n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n",
    "                # l_mask:torch.Size([8732, 4])\n",
    "\n",
    "                # l_maskをdecoded_boxesに適応します\n",
    "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
    "                # decoded_boxes[l_mask]で1次元になってしまうので、\n",
    "                # viewで（閾値を超えたBBox数, 4）サイズに変形しなおす\n",
    "\n",
    "                # 3. Non-Maximum Suppressionを実施し、被っているBBoxを取り除く\n",
    "                ids, count = nm_suppression(\n",
    "                    boxes, scores, self.nms_thresh, self.top_k)\n",
    "                # ids：confの降順にNon-Maximum Suppressionを通過したindexが格納\n",
    "                # count：Non-Maximum Suppressionを通過したBBoxの数\n",
    "\n",
    "                # outputにNon-Maximum Suppressionを抜けた結果を格納\n",
    "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1),\n",
    "                                                   boxes[ids[:count]]), 1)\n",
    "\n",
    "        return output  # torch.Size([1, 21, 200, 5])\n",
    "\n",
    "# SSDクラスを作成する\n",
    "\n",
    "\n",
    "class SSD(nn.Module):\n",
    "\n",
    "    def __init__(self, phase, cfg):\n",
    "        super(SSD, self).__init__()\n",
    "\n",
    "        self.phase = phase  # train or inferenceを指定\n",
    "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n",
    "\n",
    "        # SSDのネットワークを作る\n",
    "        self.vgg = make_vgg()\n",
    "        self.extras = make_extras()\n",
    "        self.L2Norm = L2Norm()\n",
    "        self.loc, self.conf = make_loc_conf(\n",
    "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
    "\n",
    "        # DBox作成\n",
    "        dbox = DBox(cfg)\n",
    "        self.dbox_list = dbox.make_dbox_list()\n",
    "\n",
    "        # 推論時はクラス「Detect」を用意します\n",
    "        if phase == 'inference':\n",
    "            self.detect = Detect()\n",
    "\n",
    "    def forward(self, x):\n",
    "        sources = list()  # locとconfへの入力source1～6を格納\n",
    "        loc = list()  # locの出力を格納\n",
    "        conf = list()  # confの出力を格納\n",
    "\n",
    "        # vggのconv4_3まで計算する\n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        # conv4_3の出力をL2Normに入力し、source1を作成、sourcesに追加\n",
    "        source1 = self.L2Norm(x)\n",
    "        sources.append(source1)\n",
    "\n",
    "        # vggを最後まで計算し、source2を作成、sourcesに追加\n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "\n",
    "        sources.append(x)\n",
    "\n",
    "        # extrasのconvとReLUを計算\n",
    "        # source3～6を、sourcesに追加\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if k % 2 == 1:  # conv→ReLU→cov→ReLUをしたらsourceに入れる\n",
    "                sources.append(x)\n",
    "\n",
    "        # source1～6に、それぞれ対応する畳み込みを1回ずつ適用する\n",
    "        # zipでforループの複数のリストの要素を取得\n",
    "        # source1～6まであるので、6回ループが回る\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            # Permuteは要素の順番を入れ替え\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "            # l(x)とc(x)で畳み込みを実行\n",
    "            # l(x)とc(x)の出力サイズは[batch_num, 4*アスペクト比の種類数, featuremapの高さ, featuremap幅]\n",
    "            # sourceによって、アスペクト比の種類数が異なり、面倒なので順番入れ替えて整える\n",
    "            # permuteで要素の順番を入れ替え、\n",
    "            # [minibatch数, featuremap数, featuremap数,4*アスペクト比の種類数]へ\n",
    "            # （注釈）\n",
    "            # torch.contiguous()はメモリ上で要素を連続的に配置し直す命令です。\n",
    "            # あとでview関数を使用します。\n",
    "            # このviewを行うためには、対象の変数がメモリ上で連続配置されている必要があります。\n",
    "\n",
    "        # さらにlocとconfの形を変形\n",
    "        # locのサイズは、torch.Size([batch_num, 34928])\n",
    "        # confのサイズはtorch.Size([batch_num, 183372])になる\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "\n",
    "        # さらにlocとconfの形を整える\n",
    "        # locのサイズは、torch.Size([batch_num, 8732, 4])\n",
    "        # confのサイズは、torch.Size([batch_num, 8732, 21])\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "\n",
    "        # 最後に出力する\n",
    "        output = (loc, conf, self.dbox_list)\n",
    "\n",
    "        if self.phase == \"inference\":  # 推論時\n",
    "            # クラス「Detect」のforwardを実行\n",
    "            # 返り値のサイズは torch.Size([batch_num, 21, 200, 5])\n",
    "            return self.detect(output[0], output[1], output[2])\n",
    "\n",
    "        else:  # 学習時\n",
    "            return output\n",
    "            # 返り値は(loc, conf, dbox_list)のタプル\n",
    "\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSDの損失関数のクラスです。\"\"\"\n",
    "\n",
    "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.jaccard_thresh = jaccard_thresh  # 0.5 関数matchのjaccard係数の閾値\n",
    "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Miningの負と正の比率\n",
    "        self.device = device  # CPUとGPUのいずれで計算するのか\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算。\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions : SSD netの訓練時の出力(tuple)\n",
    "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size [8732,4])。\n",
    "        targets : [num_batch, num_objs, 5]\n",
    "            5は正解のアノテーション情報[xmin, ymin, xmax, ymax, label_ind]を示す\n",
    "        Returns\n",
    "        -------\n",
    "        loss_l : テンソル\n",
    "            locの損失の値\n",
    "        loss_c : テンソル\n",
    "            confの損失の値\n",
    "        \"\"\"\n",
    "\n",
    "        # SSDモデルの出力がタプルになっているので、個々にばらす\n",
    "        loc_data, conf_data, dbox_list = predictions\n",
    "\n",
    "        # 要素数を把握\n",
    "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
    "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
    "        num_classes = conf_data.size(2)  # クラス数 = 21\n",
    "\n",
    "        # 損失の計算に使用するものを格納する変数を作成\n",
    "        # conf_t_label：各DBoxに一番近い正解のBBoxのラベルを格納させる\n",
    "        # loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "\n",
    "        # loc_tとconf_t_labelに、\n",
    "        # DBoxと正解アノテーションtargetsをmatchさせた結果を上書きする\n",
    "        for idx in range(num_batch):  # ミニバッチでループ\n",
    "\n",
    "            # 現在のミニバッチの正解アノテーションのBBoxとラベルを取得\n",
    "            truths = targets[idx][:, :-1].to(self.device)  # BBox\n",
    "            # ラベル [物体1のラベル, 物体2のラベル, …]\n",
    "            labels = targets[idx][:, -1].to(self.device)\n",
    "\n",
    "            # デフォルトボックスを新たな変数で用意\n",
    "            dbox = dbox_list.to(self.device)\n",
    "\n",
    "            # 関数matchを実行し、loc_tとconf_t_labelの内容を更新する\n",
    "            # （詳細）\n",
    "            # loc_t:各DBoxに一番近い正解のBBoxの位置情報が上書きされる\n",
    "            # conf_t_label：各DBoxに一番近いBBoxのラベルが上書きされる\n",
    "            # ただし、一番近いBBoxとのjaccard overlapが0.5より小さい場合は\n",
    "            # 正解BBoxのラベルconf_t_labelは背景クラスの0とする\n",
    "            variance = [0.1, 0.2]\n",
    "            # このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数です\n",
    "            match(self.jaccard_thresh, truths, dbox,\n",
    "                  variance, labels, loc_t, conf_t_label, idx)\n",
    "\n",
    "        # ----------\n",
    "        # 位置の損失：loss_lを計算\n",
    "        # Smooth L1関数で損失を計算する。ただし、物体を発見したDBoxのオフセットのみを計算する\n",
    "        # ----------\n",
    "        # 物体を検出したBBoxを取り出すマスクを作成\n",
    "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\n",
    "\n",
    "        # pos_maskをloc_dataのサイズに変形\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "\n",
    "        # Positive DBoxのloc_dataと、教師データloc_tを取得\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "\n",
    "        # 物体を発見したPositive DBoxのオフセット情報loc_tの損失（誤差）を計算\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # ----------\n",
    "        # クラス予測の損失：loss_cを計算\n",
    "        # 交差エントロピー誤差関数で損失を計算する。ただし、背景クラスが正解であるDBoxが圧倒的に多いので、\n",
    "        # Hard Negative Miningを実施し、物体発見DBoxと背景クラスDBoxの比が1:3になるようにする。\n",
    "        # そこで背景クラスDBoxと予想したもののうち、損失が小さいものは、クラス予測の損失から除く\n",
    "        # ----------\n",
    "        batch_conf = conf_data.view(-1, num_classes)\n",
    "\n",
    "        # クラス予測の損失を関数を計算(reduction='none'にして、和をとらず、次元をつぶさない)\n",
    "        loss_c = F.cross_entropy(\n",
    "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
    "\n",
    "        # -----------------\n",
    "        # これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
    "        # -----------------\n",
    "\n",
    "        # 物体発見したPositive DBoxの損失を0にする\n",
    "        # （注意）物体はlabelが1以上になっている。ラベル0は背景。\n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True)  # ミニバッチごとの物体クラス予測の数\n",
    "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
    "        loss_c[pos_mask] = 0  # 物体を発見したDBoxは損失0とする\n",
    "\n",
    "        # Hard Negative Miningを実施する\n",
    "        # 各DBoxの損失の大きさloss_cの順位であるidx_rankを求める\n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "\n",
    "        # （注釈）\n",
    "        # 実装コードがかなり特殊で直感的ではないです。\n",
    "        # 上記2行は、要は各DBoxに対して、損失の大きさが何番目なのかの情報を\n",
    "        # 変数idx_rankとして高速に取得したいというコードです。\n",
    "        #\n",
    "        # DBOXの損失値の大きい方から降順に並べ、DBoxの降順のindexをloss_idxに格納。\n",
    "        # 損失の大きさloss_cの順位であるidx_rankを求める。\n",
    "        # ここで、\n",
    "        # 降順になった配列indexであるloss_idxを、0から8732まで昇順に並べ直すためには、\n",
    "        # 何番目のloss_idxのインデックスをとってきたら良いのかを示すのが、idx_rankである。\n",
    "        # 例えば、\n",
    "        # idx_rankの要素0番目 = idx_rank[0]を求めるには、loss_idxの値が0の要素、\n",
    "        # つまりloss_idx[?}=0 の、?は何番かを求めることになる。ここで、? = idx_rank[0]である。\n",
    "        # いま、loss_idx[?]=0の0は、元のloss_cの要素の0番目という意味である。\n",
    "        # つまり?は、元のloss_cの要素0番目は、降順に並び替えられたloss_idxの何番目ですか\n",
    "        # を求めていることになり、 結果、\n",
    "        # ? = idx_rank[0] はloss_cの要素0番目が、降順の何番目かを示すことになる。\n",
    "\n",
    "        # 背景のDBoxの数num_negを決める。HardNegative Miningにより、\n",
    "        # 物体発見のDBoxの数num_posの3倍（self.negpos_ratio倍）とする。\n",
    "        # ただし、万が一、DBoxの数を超える場合は、DBoxの数を上限とする\n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "\n",
    "        # idx_rankは各DBoxの損失の大きさが上から何番目なのかが入っている\n",
    "        # 背景のDBoxの数num_negよりも、順位が低い（すなわち損失が大きい）DBoxを取るマスク作成\n",
    "        # torch.Size([num_batch, 8732])\n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "\n",
    "        # -----------------\n",
    "        # （終了）これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
    "        # -----------------\n",
    "\n",
    "        # マスクの形を整形し、conf_dataに合わせる\n",
    "        # pos_idx_maskはPositive DBoxのconfを取り出すマスクです\n",
    "        # neg_idx_maskはHard Negative Miningで抽出したNegative DBoxのconfを取り出すマスクです\n",
    "        # pos_mask：torch.Size([num_batch, 8732])→pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "\n",
    "        # conf_dataからposとnegだけを取り出してconf_hnmにする。形はtorch.Size([num_pos+num_neg, 21])\n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
    "                             ].view(-1, num_classes)\n",
    "        # （注釈）gtは greater than (>)の略称。これでmaskが1のindexを取り出す。\n",
    "        # pos_idx_mask+neg_idx_maskは足し算だが、indexへのmaskをまとめているだけである。\n",
    "        # つまり、posであろうがnegであろうが、マスクが1のものを足し算で一つのリストにし、それをgtで取得\n",
    "\n",
    "        # 同様に教師データであるconf_t_labelからposとnegだけを取り出してconf_t_label_hnmに\n",
    "        # 形はtorch.Size([pos+neg])になる\n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "\n",
    "        # confidenceの損失関数を計算（要素の合計=sumを求める）\n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "\n",
    "        # 物体を発見したBBoxの数N（全ミニバッチの合計）で損失を割り算\n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "\n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 考察\n",
    "FasterRCNNのネットワークではCNNのRPNで領域候補の生成を行うことによって物体候補領域の高精度化とend-to-endの学習を実現している。RPNでは、様々なアスペクト比のアンカーボックスをground truthとのIOUの値でボックスの選択と、物体と背景の分類とbounding box 回帰を学習している。一方SSDでは、RPNに代わり、挿入するCNNはExtra Feature Layersといい後段に向かうほど特徴マップの分割領域数をスケールダウンさせている。この分割領域に対して、同じくアンカーボックスを使用している。SSDではRPNと損失関数が異なり、確信度誤差のconﬁdence lossと位置特定誤差のlocalization lossを用いている。conﬁdence lossは複数クラスの確信度に対するソフトマックス誤差で、localization lossは予測されたボックスと正解ボックスのパラメータ間でのSmooth L1 lossになっている。つまり初期 bounding boxのパラメータについてオフセットを回帰予測することで学習している。そして損失関数とJaccard係数で正解に近いボックスを選択している。また、単一の予測ネットワーク中の異なる層から特徴マップを利用することで、すべての物体スケールについて同じパラメータを共有しながら，同様の効果を得ることが可能だと考えられている。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
